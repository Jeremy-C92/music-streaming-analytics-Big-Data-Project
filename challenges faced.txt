Challenge 1: Docker Networking & Kafka Advertised Listeners Problem: The external Python producer (running on Windows) 
and the internal Spark container could not both connect to Kafka using the same address. 


Solution: I configured KAFKA_ADVERTISED_LISTENERS to expose two different ports: port 9092 for external traffic (Host) 
and port 29092 for internal Docker traffic, ensuring Spark uses the internal container name kafka:29092.




Challenge 2: Data Persistence & Volume Mapping Problem: The Batch job failed to find data generated by the Streaming job (Path does not exist). 
Solution: I harmonized the Docker Volume definitions. I ensured that the Streaming job writes to the mounted volume /opt/spark/data (mapped to local ./data), 
allowing the Batch job to access the persistent Parquet files even after the stream stops.